nohup: ignoring input
[AUTO] N >= 100: forcing enc_emb_dim=1536, n_enc_heads=32, dec_emb_dim=512, n_dec_heads=4
INFO - 12/26/25 19:44:26 - 0:00:00 - ============ Initialized logger ============
INFO - 12/26/25 19:44:26 - 0:00:00 - K: 1
                                     N: 128
                                     Q: 251
                                     accumulate_gradients: 1
                                     act_ponder_coupling: 0.05
                                     act_threshold: 0.01
                                     amp: -1
                                     attention_dropout: 0
                                     balanced_base: False
                                     batch_load: False
                                     batch_size: 128
                                     batch_size_eval: 64
                                     beam_early_stopping: True
                                     beam_eval: True
                                     beam_eval_train: 0
                                     beam_length_penalty: 1
                                     beam_size: 1
                                     clip_grad_norm: 5
                                     command: python train.py --N 128 --exp_name N128_strict --save_periodic 1 --exp_id "9y92torncp"
                                     correctQ: False
                                     cpu: False
                                     debug: False
                                     debug_slurm: False
                                     dec_act: False
                                     dec_emb_dim: 512
                                     dec_gated: False
                                     dec_loop_idx: 1
                                     dec_loops: 8
                                     density: 0
                                     dropout: 0
                                     dump_path: ./checkpoint/wza/dumped/N128_strict/9y92torncp
                                     enc_act: False
                                     enc_emb_dim: 1536
                                     enc_gated: False
                                     enc_loop_idx: 1
                                     enc_loops: 2
                                     env_base_seed: 0
                                     env_name: lattice
                                     epoch_size: 300000
                                     error: True
                                     eval_data: 
                                     eval_from_exp: 
                                     eval_only: False
                                     eval_size: 1000
                                     eval_verbose: 0
                                     eval_verbose_print: False
                                     exp_id: 9y92torncp
                                     exp_name: N128_strict
                                     export_data: False
                                     fp16: False
                                     freeze_embeddings: False
                                     gated: True
                                     gelu_activation: False
                                     generator: uniform
                                     global_rank: 0
                                     hamming: 3
                                     input_int_base: 81
                                     is_master: True
                                     is_slurm_job: False
                                     local_rank: 0
                                     master_port: -1
                                     maxQ_prob: 0
                                     max_epoch: 100000
                                     max_len: 512
                                     max_output_len: 512
                                     multi_gpu: False
                                     multi_node: False
                                     n_dec_heads: 4
                                     n_dec_hidden_layers: 1
                                     n_dec_layers: 2
                                     n_enc_heads: 32
                                     n_enc_hidden_layers: 1
                                     n_enc_layers: 2
                                     n_gpu_per_node: 1
                                     n_nodes: 1
                                     no_separator: True
                                     node_id: 0
                                     norm_attention: False
                                     num_reuse_samples: 10000
                                     num_workers: 10
                                     nvidia_apex: False
                                     operation: circ_rlwe
                                     optimizer: adam_warmup,lr=0.00001,warmup_updates=8000,weight_decay=0.99
                                     output_int_base: 81
                                     percQ_bound: 1.0
                                     reload_checkpoint: 
                                     reload_data: 
                                     reload_model: 
                                     reload_size: -1
                                     reuse: True
                                     save_periodic: 1
                                     scalar_gate: False
                                     secret: 71
                                     secret_stop: True
                                     secrettype: b
                                     share_inout_emb: False
                                     sigma: 3
                                     sinusoidal_embeddings: False
                                     sparsity: 0.5
                                     stopping_criterion: _valid_lattice_xe_loss,1,75
                                     tasks: lattice
                                     times_reused: 10
                                     validation_metrics: _valid_lattice_xe_loss
                                     weighted_loss: False
                                     windows: False
                                     world_size: 1
                                     xav_init: False
INFO - 12/26/25 19:44:26 - 0:00:00 - The experiment will be stored in ./checkpoint/wza/dumped/N128_strict/9y92torncp
                                     
INFO - 12/26/25 19:44:26 - 0:00:00 - Running command: python train.py --N 128 --exp_name N128_strict --save_periodic 1

INFO - 12/26/25 19:44:26 - 0:00:00 - secrets: [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]
INFO - 12/26/25 19:44:26 - 0:00:00 - vocabulary: 86 words
INFO - 12/26/25 19:44:26 - 0:00:00 - words: {'<s>': 0, '</s>': 1, '<pad>': 2, '|': 3, '+': 4, '0': 5, '1': 6, '10': 7, '11': 8, '12': 9, '13': 10, '14': 11, '15': 12, '16': 13, '17': 14, '18': 15, '19': 16, '2': 17, '20': 18, '21': 19, '22': 20, '23': 21, '24': 22, '25': 23, '26': 24, '27': 25, '28': 26, '29': 27, '3': 28, '30': 29, '31': 30, '32': 31, '33': 32, '34': 33, '35': 34, '36': 35, '37': 36, '38': 37, '39': 38, '4': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '5': 50, '50': 51, '51': 52, '52': 53, '53': 54, '54': 55, '55': 56, '56': 57, '57': 58, '58': 59, '59': 60, '6': 61, '60': 62, '61': 63, '62': 64, '63': 65, '64': 66, '65': 67, '66': 68, '67': 69, '68': 70, '69': 71, '7': 72, '70': 73, '71': 74, '72': 75, '73': 76, '74': 77, '75': 78, '76': 79, '77': 80, '78': 81, '79': 82, '8': 83, '80': 84, '9': 85}
INFO - 12/26/25 19:44:26 - 0:00:00 - Training tasks: lattice
INFO - 12/26/25 19:44:26 - 0:00:00 - Number of parameters (encoder): 81971712
INFO - 12/26/25 19:44:26 - 0:00:00 - Number of parameters (decoder): 14791254
INFO - 12/26/25 19:44:26 - 0:00:00 - Module encoder device: cuda:0
INFO - 12/26/25 19:44:26 - 0:00:00 - Module decoder device: cuda:0
INFO - 12/26/25 19:44:26 - 0:00:00 - CUDA available: True, current device: 0, memory allocated: 387,052,032
INFO - 12/26/25 19:44:26 - 0:00:00 - Found 102 parameters in model.
INFO - 12/26/25 19:44:26 - 0:00:01 - Optimizer: <class 'src.optim.AdamWithWarmup'>
INFO - 12/26/25 19:44:26 - 0:00:01 - Creating train iterator for lattice ...
SLURM job: False
0 - Number of nodes: 1
0 - Node ID        : 0
0 - Local rank     : 0
0 - Global rank    : 0
0 - World size     : 1
0 - GPUs per node  : 1
0 - Master         : True
0 - Multi-node     : False
0 - Multi-GPU      : False
0 - Hostname       : WOOJIAN-LAB
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 0, with seed [0, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 1, with seed [1, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 4, with seed [4, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 3, with seed [3, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 5, with seed [5, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 6, with seed [6, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 8, with seed [8, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 7, with seed [7, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 2, with seed [2, 0, 0] (base seed=0).
INFO - 12/26/25 19:44:27 - 0:00:01 - ============ Starting epoch 0 ... ============
INFO - 12/26/25 19:44:27 - 0:00:01 - Initialized random generator for worker 9, with seed [9, 0, 0] (base seed=0).
Traceback (most recent call last):
  File "/home/wza/SALSA/train.py", line 365, in <module>
    main(params)
    ~~~~^^^^^^^^
  File "/home/wza/SALSA/train.py", line 289, in main
    trainer.enc_dec_step(task)
    ~~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/home/wza/SALSA/src/trainer.py", line 686, in enc_dec_step
    self.optimize(loss)
    ~~~~~~~~~~~~~^^^^^^
  File "/home/wza/SALSA/src/trainer.py", line 265, in optimize
    loss.backward()
    ~~~~~~~~~~~~~^^
  File "/home/wza/miniconda3/lib/python3.13/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        self, gradient, retain_graph, create_graph, inputs=inputs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/wza/miniconda3/lib/python3.13/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
    ~~~~~~~~~~~~~~~~~~~~^
        tensors,
        ^^^^^^^^
    ...<5 lines>...
        accumulate_grad=True,
        ^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/wza/miniconda3/lib/python3.13/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        t_outputs, *args, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    )  # Calls into the C++ engine to run the backward pass
    ^
torch.AcceleratorError: CUDA error: unknown error
Search for `cudaErrorUnknown' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

