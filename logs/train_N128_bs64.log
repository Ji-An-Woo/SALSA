nohup: ignoring input
[AUTO] N >= 100: forcing enc_emb_dim=1536, n_enc_heads=32, dec_emb_dim=512, n_dec_heads=4
INFO - 12/26/25 19:49:36 - 0:00:00 - ============ Initialized logger ============
INFO - 12/26/25 19:49:36 - 0:00:00 - K: 1
                                     N: 128
                                     Q: 251
                                     accumulate_gradients: 1
                                     act_ponder_coupling: 0.05
                                     act_threshold: 0.01
                                     amp: -1
                                     attention_dropout: 0
                                     balanced_base: False
                                     batch_load: False
                                     batch_size: 64
                                     batch_size_eval: 64
                                     beam_early_stopping: True
                                     beam_eval: True
                                     beam_eval_train: 0
                                     beam_length_penalty: 1
                                     beam_size: 1
                                     clip_grad_norm: 5
                                     command: python train.py --N 128 --exp_name N128_bs64 --batch_size 64 --save_periodic 1 --exp_id "whcq3ttn8z"
                                     correctQ: False
                                     cpu: False
                                     debug: False
                                     debug_slurm: False
                                     dec_act: False
                                     dec_emb_dim: 512
                                     dec_gated: False
                                     dec_loop_idx: 1
                                     dec_loops: 8
                                     density: 0
                                     dropout: 0
                                     dump_path: ./checkpoint/wza/dumped/N128_bs64/whcq3ttn8z
                                     enc_act: False
                                     enc_emb_dim: 1536
                                     enc_gated: False
                                     enc_loop_idx: 1
                                     enc_loops: 2
                                     env_base_seed: 0
                                     env_name: lattice
                                     epoch_size: 300000
                                     error: True
                                     eval_data: 
                                     eval_from_exp: 
                                     eval_only: False
                                     eval_size: 1000
                                     eval_verbose: 0
                                     eval_verbose_print: False
                                     exp_id: whcq3ttn8z
                                     exp_name: N128_bs64
                                     export_data: False
                                     fp16: False
                                     freeze_embeddings: False
                                     gated: True
                                     gelu_activation: False
                                     generator: uniform
                                     global_rank: 0
                                     hamming: 3
                                     input_int_base: 81
                                     is_master: True
                                     is_slurm_job: False
                                     local_rank: 0
                                     master_port: -1
                                     maxQ_prob: 0
                                     max_epoch: 100000
                                     max_len: 512
                                     max_output_len: 512
                                     multi_gpu: False
                                     multi_node: False
                                     n_dec_heads: 4
                                     n_dec_hidden_layers: 1
                                     n_dec_layers: 2
                                     n_enc_heads: 32
                                     n_enc_hidden_layers: 1
                                     n_enc_layers: 2
                                     n_gpu_per_node: 1
                                     n_nodes: 1
                                     no_separator: True
                                     node_id: 0
                                     norm_attention: False
                                     num_reuse_samples: 10000
                                     num_workers: 10
                                     nvidia_apex: False
                                     operation: circ_rlwe
                                     optimizer: adam_warmup,lr=0.00001,warmup_updates=8000,weight_decay=0.99
                                     output_int_base: 81
                                     percQ_bound: 1.0
                                     reload_checkpoint: 
                                     reload_data: 
                                     reload_model: 
                                     reload_size: -1
                                     reuse: True
                                     save_periodic: 1
                                     scalar_gate: False
                                     secret: 71
                                     secret_stop: True
                                     secrettype: b
                                     share_inout_emb: False
                                     sigma: 3
                                     sinusoidal_embeddings: False
                                     sparsity: 0.5
                                     stopping_criterion: _valid_lattice_xe_loss,1,75
                                     tasks: lattice
                                     times_reused: 10
                                     validation_metrics: _valid_lattice_xe_loss
                                     weighted_loss: False
                                     windows: False
                                     world_size: 1
                                     xav_init: False
INFO - 12/26/25 19:49:36 - 0:00:00 - The experiment will be stored in ./checkpoint/wza/dumped/N128_bs64/whcq3ttn8z
                                     
INFO - 12/26/25 19:49:36 - 0:00:00 - Running command: python train.py --N 128 --exp_name N128_bs64 --batch_size 64 --save_periodic 1

INFO - 12/26/25 19:49:36 - 0:00:00 - secrets: [array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                                            0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]
INFO - 12/26/25 19:49:36 - 0:00:00 - vocabulary: 86 words
INFO - 12/26/25 19:49:36 - 0:00:00 - words: {'<s>': 0, '</s>': 1, '<pad>': 2, '|': 3, '+': 4, '0': 5, '1': 6, '10': 7, '11': 8, '12': 9, '13': 10, '14': 11, '15': 12, '16': 13, '17': 14, '18': 15, '19': 16, '2': 17, '20': 18, '21': 19, '22': 20, '23': 21, '24': 22, '25': 23, '26': 24, '27': 25, '28': 26, '29': 27, '3': 28, '30': 29, '31': 30, '32': 31, '33': 32, '34': 33, '35': 34, '36': 35, '37': 36, '38': 37, '39': 38, '4': 39, '40': 40, '41': 41, '42': 42, '43': 43, '44': 44, '45': 45, '46': 46, '47': 47, '48': 48, '49': 49, '5': 50, '50': 51, '51': 52, '52': 53, '53': 54, '54': 55, '55': 56, '56': 57, '57': 58, '58': 59, '59': 60, '6': 61, '60': 62, '61': 63, '62': 64, '63': 65, '64': 66, '65': 67, '66': 68, '67': 69, '68': 70, '69': 71, '7': 72, '70': 73, '71': 74, '72': 75, '73': 76, '74': 77, '75': 78, '76': 79, '77': 80, '78': 81, '79': 82, '8': 83, '80': 84, '9': 85}
INFO - 12/26/25 19:49:36 - 0:00:00 - Training tasks: lattice
INFO - 12/26/25 19:49:36 - 0:00:00 - Number of parameters (encoder): 81971712
INFO - 12/26/25 19:49:36 - 0:00:00 - Number of parameters (decoder): 14791254
INFO - 12/26/25 19:49:36 - 0:00:00 - Module encoder device: cuda:0
INFO - 12/26/25 19:49:36 - 0:00:00 - Module decoder device: cuda:0
INFO - 12/26/25 19:49:36 - 0:00:00 - CUDA available: True, current device: 0, memory allocated: 387,052,032
INFO - 12/26/25 19:49:36 - 0:00:00 - Found 102 parameters in model.
INFO - 12/26/25 19:49:36 - 0:00:01 - Optimizer: <class 'src.optim.AdamWithWarmup'>
INFO - 12/26/25 19:49:36 - 0:00:01 - Creating train iterator for lattice ...
SLURM job: False
0 - Number of nodes: 1
0 - Node ID        : 0
0 - Local rank     : 0
0 - Global rank    : 0
0 - World size     : 1
0 - GPUs per node  : 1
0 - Master         : True
0 - Multi-node     : False
0 - Multi-GPU      : False
0 - Hostname       : WOOJIAN-LAB
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 0, with seed [0, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 1, with seed [1, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 4, with seed [4, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 2, with seed [2, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 3, with seed [3, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 5, with seed [5, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 6, with seed [6, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 7, with seed [7, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 8, with seed [8, 0, 0] (base seed=0).
INFO - 12/26/25 19:49:36 - 0:00:01 - ============ Starting epoch 0 ... ============
INFO - 12/26/25 19:49:36 - 0:00:01 - Initialized random generator for worker 9, with seed [9, 0, 0] (base seed=0).
/home/wza/SALSA/src/optim.py:71: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha = 1) (Triggered internally at /pytorch/torch/csrc/utils/python_arg_parser.cpp:1805.)
  exp_avg.mul_(beta1).add_(1 - beta1, grad)
INFO - 12/26/25 19:51:48 - 0:02:12 -     200 -   97.25 equations/s - 25284.19 words/s - LATTICE:  3.8229 - LR: 3.4750e-07
INFO - 12/26/25 19:53:59 - 0:04:23 -     400 -   97.88 equations/s - 25448.81 words/s - LATTICE:  2.4440 - LR: 5.9500e-07
INFO - 12/26/25 19:56:10 - 0:06:34 -     600 -   97.79 equations/s - 25426.41 words/s - LATTICE:  2.0471 - LR: 8.4250e-07
INFO - 12/26/25 19:58:19 - 0:08:44 -     800 -   98.75 equations/s - 25673.77 words/s - LATTICE:  1.9359 - LR: 1.0900e-06
INFO - 12/26/25 20:00:29 - 0:10:54 -    1000 -   98.52 equations/s - 25615.30 words/s - LATTICE:  1.9033 - LR: 1.3375e-06
INFO - 12/26/25 20:02:39 - 0:13:04 -    1200 -   98.44 equations/s - 25595.09 words/s - LATTICE:  1.8906 - LR: 1.5850e-06
INFO - 12/26/25 20:04:49 - 0:15:13 -    1400 -   98.58 equations/s - 25630.42 words/s - LATTICE:  1.8835 - LR: 1.8325e-06
INFO - 12/26/25 20:06:59 - 0:17:23 -    1600 -   98.54 equations/s - 25620.90 words/s - LATTICE:  1.8770 - LR: 2.0800e-06
INFO - 12/26/25 20:09:09 - 0:19:33 -    1800 -   98.52 equations/s - 25614.73 words/s - LATTICE:  1.8715 - LR: 2.3275e-06
INFO - 12/26/25 20:11:20 - 0:21:44 -    2000 -   97.97 equations/s - 25472.58 words/s - LATTICE:  1.8650 - LR: 2.5750e-06
INFO - 12/26/25 20:13:31 - 0:23:55 -    2200 -   97.35 equations/s - 25311.86 words/s - LATTICE:  1.8615 - LR: 2.8225e-06
